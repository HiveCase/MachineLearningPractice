{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPX1ME+wrIbSN5yNtjvgfnG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HiveCase/MachineLearningPractice/blob/main/Week8/MLP_GA8_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qflHGndQQfxK"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# uploaded = files.upload()\n",
        "import zipfile\n",
        "\n",
        "# Extract the uploaded archive.zip file\n",
        "with zipfile.ZipFile(\"archive.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"mask_dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for root, dirs, files in os.walk(\"mask_dataset\"):\n",
        "    print(root)\n",
        "    for d in dirs:\n",
        "        print(\"  📁\", d)\n",
        "    for f in files[:5]:  # Print first 5 files only\n",
        "        print(\"  📄\", f)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72QrHxDvQuP5",
        "outputId": "9b232df8-e9d5-454b-adb0-ae18180daa37"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mask_dataset\n",
            "  📁 data\n",
            "\n",
            "mask_dataset/data\n",
            "  📁 without_mask\n",
            "  📁 with_mask\n",
            "\n",
            "mask_dataset/data/without_mask\n",
            "  📄 without_mask_816.jpg\n",
            "  📄 without_mask_555.jpg\n",
            "  📄 without_mask_1209.jpg\n",
            "  📄 without_mask_1074.jpg\n",
            "  📄 without_mask_1106.jpg\n",
            "\n",
            "mask_dataset/data/with_mask\n",
            "  📄 with_mask_1687.jpg\n",
            "  📄 with_mask_1113.jpg\n",
            "  📄 with_mask_401.jpg\n",
            "  📄 with_mask_1000.jpg\n",
            "  📄 with_mask_1032.jpg\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "base_path = \"mask_dataset/data\"\n",
        "class_folders = os.listdir(base_path)\n",
        "\n",
        "for label_folder in class_folders:\n",
        "    folder_path = os.path.join(base_path, label_folder)\n",
        "\n",
        "    if os.path.isdir(folder_path):\n",
        "        for img_file in os.listdir(folder_path):\n",
        "            img_path = os.path.join(folder_path, img_file)\n",
        "\n",
        "            # Read image in grayscale\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "            if img is not None:\n",
        "                # Resize and flatten\n",
        "                img_resized = cv2.resize(img, (100, 100))\n",
        "                images.append(img_resized.flatten() / 255.0)\n",
        "                labels.append(label_folder)\n",
        "\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n"
      ],
      "metadata": {
        "id": "dRvYCZjVSjks"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "without_mask_count = np.sum(labels == 'without_mask')\n",
        "print(\"Number of 'without_mask' images:\", without_mask_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RpGq36ISnkh",
        "outputId": "956f9d55-990d-41e6-a9d3-769ab2582d17"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of 'without_mask' images: 3828\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "# Ensure correct order: without_mask → 1, with_mask → 0\n",
        "le.fit(['with_mask', 'without_mask'])\n",
        "\n",
        "y_encoded = le.transform(labels)  # 0 for with_mask, 1 for without_mask\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    images, y_encoded, test_size=0.2, random_state=0\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(\n",
        "    random_state=0,\n",
        "    max_iter=500,\n",
        "    tol=0.001,\n",
        "    C=10\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Confusion matrix: [[TN, FP], [FN, TP]]\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "false_positives = cm[0][1]\n",
        "print(\"False positives (predicted without_mask, actually with_mask):\", false_positives)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuH-wKkzSq9i",
        "outputId": "a4d28a03-13d3-4065-b861-05b92f9aeaac"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False positives (predicted without_mask, actually with_mask): 274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage import rotate\n",
        "\n",
        "def augment_image(images, labels, angles, augmentation_factor):\n",
        "    np.random.seed(0)  # Reproducibility\n",
        "\n",
        "    num_images = len(images)\n",
        "    img_size = int(np.sqrt(images.shape[1]))  # assuming square images e.g., 100x100\n",
        "\n",
        "    # Reshape flat images to 2D (100, 100)\n",
        "    images_reshaped = images.reshape((-1, img_size, img_size))\n",
        "\n",
        "    augmented_images_list = []\n",
        "    augmented_labels_list = []\n",
        "\n",
        "    for i in range(num_images):\n",
        "        # Add original image and label\n",
        "        augmented_images_list.append(images_reshaped[i])\n",
        "        augmented_labels_list.append(labels[i])\n",
        "\n",
        "        for j in range(augmentation_factor):\n",
        "            angle_index = i * augmentation_factor + j\n",
        "            angle = angles[angle_index]\n",
        "\n",
        "            rotated_img = rotate(images_reshaped[i], angle, reshape=False, mode='nearest')\n",
        "            augmented_images_list.append(rotated_img)\n",
        "            augmented_labels_list.append(labels[i])\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    augmented_images = np.array(augmented_images_list)\n",
        "    augmented_labels = np.array(augmented_labels_list)\n",
        "\n",
        "    # Reshape images back to flattened form (optional, if your model expects flat input)\n",
        "    augmented_images = augmented_images.reshape((augmented_images.shape[0], -1))\n",
        "\n",
        "    return augmented_images, augmented_labels\n",
        "\n",
        "# Set augmentation factor\n",
        "augmentation_factor = 2\n",
        "\n",
        "# Generate angles for rotation\n",
        "np.random.seed(0)\n",
        "num_train_images = len(X_train)\n",
        "angle_of_rotation = np.random.uniform(-180, 180, size=(augmentation_factor * num_train_images,))\n",
        "\n",
        "# Call augment_image function\n",
        "augmented_images, augmented_labels = augment_image(\n",
        "    X_train, y_train, angle_of_rotation, augmentation_factor\n",
        ")\n",
        "\n",
        "# Compute the sum of elements in augmented_labels[7000:8000]\n",
        "sum_labels = np.sum(augmented_labels[7000:8000])\n",
        "print(\"Sum of augmented_labels from index 7000 to 7999:\", sum_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrfqSk3oS57H",
        "outputId": "e9e2f273-21f7-4967-dbaf-00492974caa7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of augmented_labels from index 7000 to 7999: 485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Reshape augmented images to 2D (samples, features)\n",
        "X_aug = augmented_images.reshape((augmented_images.shape[0], -1))\n",
        "y_aug = augmented_labels\n",
        "\n",
        "# Step 2: Train RandomForest on all features\n",
        "rf = RandomForestClassifier(random_state=0)\n",
        "rf.fit(X_aug, y_aug)\n",
        "\n",
        "# Step 3: Get top 100 important features\n",
        "importances = rf.feature_importances_\n",
        "top_100_indices = np.argsort(importances)[-100:]\n",
        "\n",
        "# Step 4: Select top 100 features from augmented training and test sets\n",
        "X_aug_top100 = X_aug[:, top_100_indices]\n",
        "X_test_reshaped = X_test.reshape((X_test.shape[0], -1))\n",
        "X_test_top100 = X_test_reshaped[:, top_100_indices]\n",
        "\n",
        "# Step 5: Train on selected features\n",
        "rf_top100 = RandomForestClassifier(random_state=0)\n",
        "rf_top100.fit(X_aug_top100, y_aug)\n",
        "\n",
        "# Step 6: Predict and calculate misclassifications\n",
        "y_pred = rf_top100.predict(X_test_top100)\n",
        "misclassified_count = np.sum(y_pred != y_test)\n",
        "\n",
        "print(\"Number of misclassified test images:\", misclassified_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJs1rWGtS-_j",
        "outputId": "75426ab8-1d93-465c-b15f-0205936612a7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of misclassified test images: 282\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Flatten images\n",
        "X_aug_flat = augmented_images.reshape((augmented_images.shape[0], -1))\n",
        "X_test_flat = X_test.reshape((X_test.shape[0], -1))\n",
        "\n",
        "# Step 2: Apply PCA\n",
        "pca = PCA(n_components=100, random_state=0)\n",
        "X_aug_pca = pca.fit_transform(X_aug_flat)\n",
        "X_test_pca = pca.transform(X_test_flat)\n",
        "\n",
        "# Step 3: Train RandomForest on PCA-transformed data\n",
        "rf_pca = RandomForestClassifier(random_state=0)\n",
        "rf_pca.fit(X_aug_pca, augmented_labels)\n",
        "\n",
        "# Step 4: Predict and compute accuracy\n",
        "y_pred = rf_pca.predict(X_test_pca)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy on test data after PCA + RandomForest:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNynLBniTBUr",
        "outputId": "562e7c65-d045-4201-abe5-1f321d66c5ae"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test data after PCA + RandomForest: 0.786896095301125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z4zeWbwbTN3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fBH7Fx7vVoBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iOh27rghVn-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import libraries\n",
        "import zipfile\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Step 2: Unzip the file (assuming it's in the root `/content` folder)\n",
        "with zipfile.ZipFile(\"/content/archive.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/dataset\")\n",
        "\n",
        "# Step 3: Prepare lists for images and labels\n",
        "image_list = []\n",
        "label_list = []\n",
        "\n",
        "# Step 4: Traverse the dataset directory\n",
        "dataset_path = \"/content/dataset\"\n",
        "for label_name in os.listdir(dataset_path):\n",
        "    label_folder = os.path.join(dataset_path, label_name)\n",
        "    if os.path.isdir(label_folder):\n",
        "        for image_file in tqdm(os.listdir(label_folder), desc=f\"Processing {label_name}\"):\n",
        "            image_path = os.path.join(label_folder, image_file)\n",
        "            try:\n",
        "                # Read image in grayscale\n",
        "                img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "                if img is None:\n",
        "                    continue  # skip unreadable files\n",
        "\n",
        "                # Resize to 100x100\n",
        "                img_resized = cv2.resize(img, (100, 100))\n",
        "\n",
        "                # Normalize pixel values\n",
        "                img_normalized = img_resized / 255.0\n",
        "\n",
        "                # Flatten and append\n",
        "                image_list.append(img_normalized.flatten())\n",
        "                label_list.append(label_name)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {image_file}: {e}\")\n",
        "\n",
        "# Step 5: Convert to numpy arrays\n",
        "images = np.array(image_list)\n",
        "labels = np.array(label_list)\n",
        "\n",
        "# Step 6: Count images with label \"without_mask\"\n",
        "without_mask_count = np.sum(labels == \"without_mask\")\n",
        "print(\"Number of images with label 'without_mask':\", without_mask_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_1AzgviVn81",
        "outputId": "3e77efc0-7a69-4b10-9c78-ed2b28d9d88a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing data: 100%|██████████| 2/2 [00:00<00:00, 3833.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images with label 'without_mask': 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Step 2: Encode labels (without_mask → 1, with_mask → 0)\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Ensure correct mapping\n",
        "mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "print(\"Label mapping:\", mapping)\n",
        "# Expected Output: {'with_mask': 0, 'without_mask': 1}\n",
        "\n",
        "# Step 3: Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    images, labels_encoded, test_size=0.2, random_state=0\n",
        ")\n",
        "\n",
        "# Step 4: Train Logistic Regression\n",
        "model = LogisticRegression(\n",
        "    random_state=0,\n",
        "    max_iter=500,\n",
        "    tol=0.001,\n",
        "    C=10\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict and calculate confusion matrix\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Confusion matrix layout:\n",
        "# [[TN FP]\n",
        "#  [FN TP]]\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "false_positives = cm[0][1]\n",
        "print(\"Number of false positives (with_mask → predicted without_mask):\", false_positives)\n"
      ],
      "metadata": {
        "id": "axQYvtH-VreZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Define the augmentation function\n",
        "def augment_image(images, labels, angles, augmentation_factor):\n",
        "    augmented_images = []\n",
        "    augmented_labels = []\n",
        "\n",
        "    idx = 0\n",
        "    for i in range(len(images)):\n",
        "        original_img = images[i].reshape(100, 100)  # Reshape flattened image\n",
        "        original_label = labels[i]\n",
        "\n",
        "        # Append the original image and label\n",
        "        augmented_images.append(original_img)\n",
        "        augmented_labels.append(original_label)\n",
        "\n",
        "        # Generate augmented images by rotation\n",
        "        for _ in range(augmentation_factor):\n",
        "            angle = angles[idx]\n",
        "            M = cv2.getRotationMatrix2D((50, 50), angle, 1.0)  # center=(50,50) for 100x100\n",
        "            rotated = cv2.warpAffine(original_img, M, (100, 100), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT)\n",
        "            augmented_images.append(rotated)\n",
        "            augmented_labels.append(original_label)\n",
        "            idx += 1\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    augmented_images = np.array(augmented_images).reshape(-1, 100 * 100).astype(np.float32)\n",
        "    augmented_labels = np.array(augmented_labels)\n",
        "\n",
        "    return augmented_images, augmented_labels\n",
        "\n",
        "# Set random seed and generate angles\n",
        "np.random.seed(0)\n",
        "augmentation_factor = 2\n",
        "angle_of_rotation = np.random.uniform(-180, 180, size=augmentation_factor * len(X_train))\n",
        "\n",
        "# Call the function\n",
        "aug_images, aug_labels = augment_image(X_train, y_train, angle_of_rotation, augmentation_factor)\n",
        "\n",
        "# Compute the sum of elements in augmented_labels[7000:8000]\n",
        "label_sum = np.sum(aug_labels[7000:8000])\n",
        "print(\"Sum of augmented_labels[7000:8000]:\", label_sum)\n"
      ],
      "metadata": {
        "id": "yrDWo4nRVzo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Step 1: Fit RandomForest on augmented training data\n",
        "rf = RandomForestClassifier(random_state=0)\n",
        "rf.fit(aug_images, aug_labels)\n",
        "\n",
        "# Step 2: Get top 100 feature indices based on feature_importances_\n",
        "importances = rf.feature_importances_\n",
        "top_100_indices = np.argsort(importances)[-100:]\n",
        "\n",
        "# Step 3: Select only top 100 features from training and test data\n",
        "aug_images_top100 = aug_images[:, top_100_indices]\n",
        "X_test_top100 = X_test[:, top_100_indices]\n",
        "\n",
        "# Step 4: Train new RandomForest on selected features\n",
        "rf_top100 = RandomForestClassifier(random_state=0)\n",
        "rf_top100.fit(aug_images_top100, aug_labels)\n",
        "\n",
        "# Step 5: Predict and count misclassified points on test data\n",
        "y_pred_test = rf_top100.predict(X_test_top100)\n",
        "misclassified = np.sum(y_pred_test != y_test)\n",
        "\n",
        "print(\"Number of misclassified test images using top 100 features:\", misclassified)\n"
      ],
      "metadata": {
        "id": "4rx7RUv4WZ7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Fit PCA on augmented training images\n",
        "pca = PCA(n_components=100, random_state=0)\n",
        "aug_images_pca = pca.fit_transform(aug_images)\n",
        "\n",
        "# Step 2: Transform test images using the same PCA\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# Step 3: Train RandomForestClassifier\n",
        "rf_pca = RandomForestClassifier(random_state=0)\n",
        "rf_pca.fit(aug_images_pca, aug_labels)\n",
        "\n",
        "# Step 4: Predict and compute accuracy\n",
        "y_pred_pca = rf_pca.predict(X_test_pca)\n",
        "accuracy = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "print(\"Accuracy on test data after PCA + RandomForest:\", round(accuracy, 4))\n"
      ],
      "metadata": {
        "id": "oRJaQhWiWa2s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}